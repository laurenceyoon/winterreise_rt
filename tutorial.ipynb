{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "*winterreise_rt* is a subset of Schubert Winterreise Dataset(SWD) [1] for real-time lyrics alignment.\n",
    "The Schubert Winterreise Dataset(SWD) dataset is a collection of resources of Schubert’s song cycle for voice and piano ‘Winterreise’. \n",
    "The the song cycle Winterreise D911 (Op. 89) consists of 24 songs composed for solo voice with piano accompaniment.\n",
    "\n",
    "## Download the latest version of the Schubert Winterreise Dataset (SWD)\n",
    "First of all, we will download the SWD dataset and revise the file structure to fit the *winterreise_rt* dataset from the scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://zenodo.org/record/5139893/files/Schubert_Winterreise_Dataset_v2-0.zip?download=1\" -O winterreise.zip\n",
    "!unzip winterreise -d winterreise\n",
    "!rm -r winterreise.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import music21\n",
    "from music21 import converter\n",
    "import numpy as np\n",
    "import scipy\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from synctoolbox.dtw.mrmsdtw import sync_via_mrmsdtw\n",
    "from synctoolbox.feature.dlnco import pitch_onset_features_to_DLNCO\n",
    "from synctoolbox.feature.pitch_onset import audio_to_pitch_onset_features\n",
    "from synctoolbox.feature.utils import estimate_tuning\n",
    "\n",
    "from reconstruct import main as reconstruct_main\n",
    "\n",
    "SWD_PATH = Path(\"./winterreise\")\n",
    "WINTERREISE_RT_PATH = Path(\"./winterreise_rt\")\n",
    "AUDIO_DIR = \"01_RawData/audio_wav/\"\n",
    "SCORE_DIR = \"01_RawData/score_musicxml/\"\n",
    "LYRICS_DIR = \"01_RawData/lyrics_txt/\"\n",
    "NOTE_ANN_DIR = \"02_Annotations/ann_audio_note/\"\n",
    "AUDIO_KEY_ANN_DIR = \"03_ExtraMaterial/\"\n",
    "FILENAME_PREFIX = \"Schubert_D911-\"\n",
    "SINGERS = {\n",
    "    \"HU33\": \"ref\",\n",
    "    \"SC06\": \"target\",\n",
    "}  # singer_name: role\n",
    "SONG_IDS = [\"{:02d}\".format(i) for i in range(1, 25)]  # 01 ~ 24\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_RATE = 25\n",
    "HOP_LENGTH = 640\n",
    "TOLERANCES = [200, 300, 500, 750, 1000]\n",
    "N_FRAMES = 3000\n",
    "THRESHOLD_REC = 10**6\n",
    "STEP_WEIGHTS = np.array([1.5, 1.5, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct the winterreise_rt dataset\n",
    "reconstruct_main(Path(\"./winterreise\"), Path(\"./winterreise_rt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Extract Lyrics Annotations from the Symbolic Music Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_ann_info_from_note_obj(\n",
    "    annots, note, offset, quarter_tempo, last_pitch, lyrics\n",
    "):\n",
    "    start_timestamp = offset * (60 / quarter_tempo)\n",
    "    pitch = int(note.pitch.ps)\n",
    "    pitch_class = note.pitch.pitchClass\n",
    "    pitch_name = note.pitch.nameWithOctave\n",
    "    lyric = (\n",
    "        note.lyrics[0].text.strip(\"- \") if note.lyrics else \"\"\n",
    "    )  # assume there's only 1 verse\n",
    "    syllabic = (\n",
    "        note.lyrics[0].syllabic if note.lyrics else \"none\"\n",
    "    )  # assume there's only 1 verse\n",
    "    if (\n",
    "        lyric == \"\"\n",
    "        and pitch == last_pitch\n",
    "        and note.tie is not None\n",
    "        and note.tie.type == \"stop\"\n",
    "    ):  # omit tie stop note with no lyric (omitted in ref annotation)\n",
    "        print(f\"skip tie note! index: {len(annots)}\")\n",
    "        return annots, last_pitch, lyrics\n",
    "    \n",
    "    last_pitch = pitch\n",
    "    annots.append(\n",
    "        {\n",
    "            \"start\": start_timestamp,\n",
    "            \"pitch\": pitch,\n",
    "            \"pitchclass\": pitch_class,\n",
    "            \"pitchname\": pitch_name,\n",
    "            \"offset\": offset,\n",
    "            \"instrument\": \"voice\",\n",
    "            \"lyric\": lyric,\n",
    "            \"syllabic\": syllabic,\n",
    "        }\n",
    "    )\n",
    "    lyrics += lyric + \" \" if syllabic in {\"single\", \"end\"} else lyric\n",
    "    return annots, last_pitch, lyrics\n",
    "\n",
    "def extract_annots_lyrics_from_score(xml_path):\n",
    "    c = converter.parse(xml_path.as_posix())\n",
    "    voice = c.parts[0]\n",
    "\n",
    "    _, _, mm = c.metronomeMarkBoundaries()[0]\n",
    "    quarter_tempo = (\n",
    "        mm.number * mm.referent.quarterLength\n",
    "    )  # normalize tempo to quarter note\n",
    "    print(f\"mm: {mm}, mm.number: {mm.number}, quarter_tempo: {quarter_tempo}\")\n",
    "    annots = []\n",
    "    lyrics = \"\"\n",
    "    last_pitch = 0\n",
    "    for el in voice.recurse():\n",
    "        if isinstance(el, music21.chord.Chord):\n",
    "            offset = el.activeSite.offset + el.offset\n",
    "            for note in el.notes:\n",
    "                annots, last_pitch, lyrics = _update_ann_info_from_note_obj(\n",
    "                    annots, note, offset, quarter_tempo, last_pitch, lyrics\n",
    "                )\n",
    "        elif isinstance(el, music21.stream.Voice):\n",
    "            measure_offset = el.activeSite.offset\n",
    "            for element in el.elements:\n",
    "                if isinstance(element, music21.note.Note):\n",
    "                    offset = element.offset + measure_offset\n",
    "                    annots, last_pitch, lyrics = _update_ann_info_from_note_obj(\n",
    "                        annots, element, offset, quarter_tempo, last_pitch, lyrics\n",
    "                    )\n",
    "        elif isinstance(el, music21.note.Note):\n",
    "            if el.activeSite.offset == 0 and isinstance(\n",
    "                el.activeSite, music21.stream.Voice\n",
    "            ):\n",
    "                continue\n",
    "            offset = el.activeSite.offset + el.offset\n",
    "            annots, last_pitch, lyrics = _update_ann_info_from_note_obj(\n",
    "                annots, el, offset, quarter_tempo, last_pitch, lyrics\n",
    "            )\n",
    "    annots = sorted(annots, key=lambda x: x[\"start\"])\n",
    "    return annots, lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots, lyrics = extract_annots_lyrics_from_score(WINTERREISE_RT_PATH / SCORE_DIR / f\"{FILENAME_PREFIX}01.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Offline Lyrics Alignment and Evaluation between *score* and *ref* in Voice Note-Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_strict_alignment_path_mask(P):\n",
    "    P = np.array(P, copy=True)\n",
    "    N, M = P[-1]\n",
    "    # Get indices for strict monotonicity\n",
    "    keep_mask = (P[1:, 0] > P[:-1, 0]) & (P[1:, 1] > P[:-1, 1])\n",
    "    # Add first index to enforce start boundary condition\n",
    "    keep_mask = np.concatenate(([True], keep_mask))\n",
    "    # Remove all indices for of last row or column\n",
    "    keep_mask[(P[:, 0] == N) | (P[:, 1] == M)] = False\n",
    "    # Add last index to enforce end boundary condition\n",
    "    keep_mask[-1] = True\n",
    "    P_mod = P[keep_mask, :]\n",
    "\n",
    "    return P_mod\n",
    "\n",
    "def make_path_strictly_monotonic(P: np.ndarray) -> np.ndarray:\n",
    "    return compute_strict_alignment_path_mask(P.T)\n",
    "\n",
    "def transfer_note_positions(wp, note_ann_1, feature_rate=FRAME_RATE):\n",
    "    x, y = wp[0] / feature_rate, wp[1] / feature_rate\n",
    "    f = scipy.interpolate.interp1d(x, y, kind=\"linear\")\n",
    "    note_positions_1_transferred_to_2 = f(note_ann_1)\n",
    "    return note_positions_1_transferred_to_2\n",
    "\n",
    "def get_stats(\n",
    "    wp,\n",
    "    note_ann_filepath_1,\n",
    "    note_ann_filepath_2,\n",
    "    feature_rate=FRAME_RATE,\n",
    "    tolerances=TOLERANCES,\n",
    "):  # tolerances in milliseconds\n",
    "    wp = make_path_strictly_monotonic(wp)\n",
    "\n",
    "    note_ann_1 = pd.read_csv(filepath_or_buffer=note_ann_filepath_1, delimiter=\",\")[\n",
    "        \"start\"\n",
    "    ]\n",
    "    note_ann_2 = pd.read_csv(filepath_or_buffer=note_ann_filepath_2, delimiter=\",\")[\n",
    "        \"start\"\n",
    "    ]\n",
    "\n",
    "    note_positions_1_transferred_to_2 = transfer_note_positions(\n",
    "        wp, note_ann_1, feature_rate\n",
    "    )\n",
    "\n",
    "    absolute_errors_at_voice_notes = np.abs(\n",
    "        note_ann_2 - note_positions_1_transferred_to_2\n",
    "    )\n",
    "    errors_at_voice_notes = note_ann_2 - note_positions_1_transferred_to_2\n",
    "\n",
    "    misalignments = np.zeros(len(tolerances))\n",
    "\n",
    "    for idx, tolerance in enumerate(tolerances):  # in milliseconds\n",
    "        misalignments[idx] = np.mean(\n",
    "            (absolute_errors_at_voice_notes > tolerance / 1000.0)\n",
    "        )\n",
    "\n",
    "    mean = np.mean(absolute_errors_at_voice_notes) * 1000.0\n",
    "    std = np.std(absolute_errors_at_voice_notes) * 1000.0\n",
    "\n",
    "    return (\n",
    "        mean,\n",
    "        std,\n",
    "        np.array(misalignments),\n",
    "        errors_at_voice_notes,\n",
    "        absolute_errors_at_voice_notes,\n",
    "    )\n",
    "\n",
    "def _get_DLNCO_features_from_audio(\n",
    "    audio,\n",
    "    tuning_offset,\n",
    "    feature_sequence_length,\n",
    "    Fs=SAMPLE_RATE,\n",
    "    feature_rate=FRAME_RATE,\n",
    "    verbose=False,\n",
    "):\n",
    "    f_pitch_onset = audio_to_pitch_onset_features(\n",
    "        f_audio=audio, Fs=Fs, tuning_offset=tuning_offset, verbose=verbose\n",
    "    )\n",
    "\n",
    "    f_DLNCO = pitch_onset_features_to_DLNCO(\n",
    "        f_peaks=f_pitch_onset,\n",
    "        feature_rate=feature_rate,\n",
    "        feature_sequence_length=feature_sequence_length,\n",
    "        visualize=verbose,\n",
    "    )\n",
    "\n",
    "    return f_DLNCO\n",
    "\n",
    "\n",
    "def run_offline_alignment(score_audio_path: Path, ref_audio_path: Path):\n",
    "    # read audio\n",
    "    audio_1, _ = librosa.load(score_audio_path.as_posix(), sr=SAMPLE_RATE)\n",
    "    audio_2, _ = librosa.load(ref_audio_path.as_posix(), sr=SAMPLE_RATE)\n",
    "\n",
    "    # estimate tuning\n",
    "    tuning_offset_1 = estimate_tuning(audio_1, SAMPLE_RATE, N=HOP_LENGTH * 2)\n",
    "    tuning_offset_2 = estimate_tuning(audio_2, SAMPLE_RATE, N=HOP_LENGTH * 2)\n",
    "\n",
    "    # generate chroma features from librosa\n",
    "    f_chroma_librosa_1 = librosa.feature.chroma_cens(\n",
    "        y=audio_1,\n",
    "        sr=SAMPLE_RATE,\n",
    "        hop_length=HOP_LENGTH,\n",
    "    )\n",
    "    f_chroma_librosa_2 = librosa.feature.chroma_cens(\n",
    "        y=audio_2,\n",
    "        sr=SAMPLE_RATE,\n",
    "        hop_length=HOP_LENGTH,\n",
    "    )\n",
    "\n",
    "    # generate DLNCO features\n",
    "    f_DLNCO_1 = _get_DLNCO_features_from_audio(\n",
    "        audio=audio_1,\n",
    "        tuning_offset=tuning_offset_1,\n",
    "        feature_sequence_length=f_chroma_librosa_1.shape[1],\n",
    "    )\n",
    "\n",
    "    f_DLNCO_2 = _get_DLNCO_features_from_audio(\n",
    "        audio=audio_2,\n",
    "        tuning_offset=tuning_offset_2,\n",
    "        feature_sequence_length=f_chroma_librosa_2.shape[1],\n",
    "    )\n",
    "    wp_chroma_dlnco = sync_via_mrmsdtw(\n",
    "        f_chroma1=f_chroma_librosa_1,\n",
    "        f_onset1=f_DLNCO_1,\n",
    "        f_chroma2=f_chroma_librosa_2,\n",
    "        f_onset2=f_DLNCO_2,\n",
    "        input_feature_rate=FRAME_RATE,\n",
    "        step_weights=STEP_WEIGHTS,\n",
    "        threshold_rec=THRESHOLD_REC,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return wp_chroma_dlnco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline evaluation (score vs. reference)\n",
    "for song_id in tqdm(SONG_IDS):\n",
    "    score_audio_path = Path(\"score/audio_wav/\") / f\"audio_{FILENAME_PREFIX}{song_id}_score.wav\"\n",
    "    ref_audio_path = WINTERREISE_RT_PATH / AUDIO_DIR / f\"{FILENAME_PREFIX}{song_id}_ref.wav\"\n",
    "    wp = run_offline_alignment(score_audio_path, ref_audio_path)\n",
    "    # wp_file = WINTERREISE_RT_PATH / WP_ANN_DIR / f\"wp_{FILENAME_PREFIX}{song_id}_offline.csv\"\n",
    "    note_ann_file_1 = Path(\"score/ann_audio_note/\") / f\"ann_{FILENAME_PREFIX}{song_id}_score.csv\"\n",
    "    note_ann_file_2 = WINTERREISE_RT_PATH / NOTE_ANN_DIR / f\"ann_{FILENAME_PREFIX}{song_id}_ref.csv\"\n",
    "\n",
    "    note_ann_1 = pd.read_csv(filepath_or_buffer=note_ann_file_1.as_posix(), delimiter=\",\")[\n",
    "        \"start\"\n",
    "    ]\n",
    "    note_ann_2 = pd.read_csv(filepath_or_buffer=note_ann_file_2.as_posix(), delimiter=\",\")[\n",
    "        \"start\"\n",
    "    ]\n",
    "\n",
    "    (\n",
    "        offline_mean,\n",
    "        offline_std,\n",
    "        offline_misalignments,\n",
    "        offline_abs_err,\n",
    "        offline_err,\n",
    "    ) = get_stats(\n",
    "        wp=wp,\n",
    "        note_ann_filepath_1=note_ann_file_1.as_posix(),\n",
    "        note_ann_filepath_2=note_ann_file_2.as_posix(),\n",
    "        feature_rate=FRAME_RATE,\n",
    "        tolerances=TOLERANCES,\n",
    "    )\n",
    "    stats_dict = {song_id: dict()}\n",
    "    stats_dict[song_id][\"chroma_dlnco\"] = {\n",
    "        \"mean\": offline_mean,\n",
    "        \"std\": offline_std,\n",
    "        \"misalignments\": offline_misalignments,\n",
    "        \"absolute_errors\": offline_abs_err,\n",
    "    }\n",
    "\n",
    "    rows = pd.MultiIndex.from_product([stats_dict.keys()], names=[\"Song ID\"])\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [[\"Chroma & DLNCO\"], TOLERANCES], names=[\"Feature Type\", \"$\\u03C4$ (ms)\"]\n",
    "    )\n",
    "    data = np.zeros((len(stats_dict), len(offline_misalignments)))\n",
    "    for row_idx, song_id in enumerate(stats_dict):\n",
    "        data[row_idx, : len(offline_misalignments)] = (\n",
    "            stats_dict[song_id][\"chroma_dlnco\"][\"misalignments\"] * 100\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(data, index=rows, columns=columns)\n",
    "    with pd.option_context(\"display.float_format\", \"{:0.2f}\".format):\n",
    "        ipd.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] C. Weiß, F. Zalkow, V. Arifi-Müller, M. Müller, H. V.Koops, A. Volk, and H. Grohganz, “Schubert Winterreise dataset: A multimodal scenario for music analysis,” ACM Journal on Computing and Cultural Heritage (JOCCH), vol. 14, no. 2, pp. 25:1–18, 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
